#progress notes
#made a lot of progress with this but broke it at the last minute, so it's currently not working
#it was pretty damn close though
#should the output format be a csv, json, or linked up with a google sheet?
#or should it be its own lil database?
#also, finish the pickling stuff: it doesn't need to be reloaded if it's been reloaded that day (or maybe in the last week? wish there was a way to partial reload)
#I haven't used AO3Scraper at all (just AO3_api) - should that be deleted?
#maybe add a "my fandoms" file. maybe that is most useful if I use this as a google sheet

#goal:
#go to a user's bookmarks
#pull a list of all bookmarked authors
#check the author's fics
#return a list of all their fandoms
#excluding any already bookmarked (for that author specifically)
#export as csv?

#remaining qs:
#is json easier than nesting dicts? probably?
#try pickling
#how to handle fics with multiple authors 
#or fics with pseuds
#run in background?
#handle rate limiting errors


#start: bookmarks (user object)
#get list of bookmarks (list of work objects)
# get list of bookmarked authors (list of user objects)
    # if logged in
        #start: subscribed authors
#get authors of interest (list of user objects - combined bookmarked/subscribed)
    #output type: user object (keep for display: username as dict key)
#for each author: get works
    #output type: work object 
#for each work: get fandoms (keep for display: [fandoms])

import AO3
from configparser import ConfigParser
import json
import csv
import time
import random
import AO3

def start():
    #AO3.utils.limit_requests(True) #this makes even simple requests take a super long time, but it's probably necessary for larger queries
    #AO3.utils.load_fandoms() #is this necessary? or do they just come as metadata attributes from work objects?
    type = input("login or guest? ")
    if type == "login":
        username = input("username: ")
        config = ConfigParser()
        config.read('config.ini')
        if username == config.get('AO3','username'):
            password = config.get('AO3', 'password')
        else:
            password = input("password: ")
        session = AO3.Session(username, password)
        print(f"Welcome, {username}")
    else:
        session = "guest"
        print("Welcome, Guest")
    return session

def get_subscribed_authors(session):
    if session == "guest":
        pass
    else:
        subscribed_authors = session.get_user_subscriptions()
        return subscribed_authors

def get_bookmarks(session):
    if session == "guest":
        user = input("What user would you like to look up?: ")
        bookmarks = user.get_bookmarks()
        #AttributeError: 'str' has no attribute 'get_bookmarks'
        return bookmarks
    else:
        user = session.user
        bookmarks = session.get_bookmarks()
        return bookmarks
    
def get_bookmarks_with_strategies(session, cache=None):
    if cache is None:
        cache = {}  # Initialize an empty cache
        
    # Check if bookmarks are already cached
    if 'bookmarks' in cache:
        return cache['bookmarks']
    
    # Initialize empty list to store all bookmarks
    all_bookmarks = []
    
    # Define batch size for batching requests
    batch_size = 30  # Adjust batch size based on API rate limits
    
    # Initialize variables for exponential backoff
    retries = 0
    max_retries = 5  # Adjust maximum number of retries
    base_delay = 1  # Initial delay in seconds
    
    while retries <= max_retries:
        try:
            # Make API call to get bookmarks
            bookmarks = session.get_bookmarks()
            
            # Group bookmarks into batches
            for i in range(0, len(bookmarks), batch_size):
                bookmarks_batch = bookmarks[i:i+batch_size]
                
                # Process each batch of bookmarks
                for bookmark in bookmarks_batch:
                    # Perform any necessary processing on the bookmark object
                    bookmark.reload()
                    pass
                
                all_bookmarks.extend(bookmarks_batch)
                
            # Cache the bookmarks
            cache['bookmarks'] = all_bookmarks
            
            return all_bookmarks
        
        except Exception as e:
            print(f"Error fetching bookmarks: {e}")
            retries += 1
            delay = base_delay * 2 ** retries  # Exponential backoff
            time.sleep(delay + random.uniform(0, 1))  # Add jitter to avoid synchronized retries

    # If all retries fail, raise an exception
    raise Exception("Failed to fetch bookmarks after multiple retries")
    
def get_bookmarked_fandoms(bookmarks):
    # Collect unique fandoms into a set
    unique_fandoms_set = set()
    for work in bookmarks:
        unique_fandoms_set.update(work.fandoms)

    # Convert the set back to a list
    bookmarked_fandoms = list(unique_fandoms_set)

    return bookmarked_fandoms

def get_bookmarked_authors(bookmarks):
    bookmarked_authors = []
    for work in bookmarks:
        for author in work.authors:
            if work.authors not in bookmarked_authors:
                bookmarked_authors.append(author)
    return bookmarked_authors

def merge_authors(subscribed_authors, bookmarked_authors):
    merged_authors = []
    merged_authors = subscribed_authors + bookmarked_authors
    return merged_authors

def filter_unique(merged_authors):
    authors = []
    unique_values = set()
    for author in merged_authors:
        key = (author.username)
        if key not in unique_values:
            authors.append(author)
            unique_values.add(key)
    return authors

def get_author_fandoms(authors, file_path):
    # Load the author_last_reload dictionary
    # this will need to be rewritten once I decide which data storage filetype to go with
    from pickling import load_author_last_reload
    author_last_reload = load_author_last_reload(file_path)
    
    result_dict = {}
    from datetime import date
    today = date.today()  # Corrected
    from datetime import timedelta
    last_week = today - timedelta(days=7)  # Date from one week ago
    
    for author in authors:
        try:
            # Check if author has been reloaded within the last week
            last_reload = author_last_reload.get(author.username)
            #need to add: if we're not reloading, pull the pickled user object & use that instead
            if last_reload is None or last_reload < last_week:
                author.reload()
                author_last_reload[author.username] = today
                reload_date_str = today.strftime('%m/%d/%Y')
                
                # Introduce a 5-second delay after reloading the author
                from time import sleep as time_sleep
                time_sleep(5)
            else:
                reload_date_str = last_reload.strftime('%m/%d/%Y') if last_reload else ''
            
            authorworks = author.get_works()
            
            fandoms_set = set()
            for work in authorworks:
                fandoms_set.update(work.fandoms)
            
            # Find the work with the most recent date_updated
            most_recent_work = max(authorworks, key=lambda x: x.date_updated)
            last_updated = most_recent_work.date_updated.strftime('%m/%d/%Y') if authorworks else None
            
            result_dict[author.username] = {'author': author.username, 'fandoms': list(fandoms_set), 'last_updated': last_updated, 'last_reload_date': reload_date_str}
        except Exception as e:
            print(f"Error processing {author.username}: {e}")
            continue
    
    # Save the author_last_reload dictionary
    #this will need to be rewritten
    from pickling import save_author_last_reload
    save_author_last_reload(author_last_reload, file_path)
    
    return result_dict

def find_common_fandoms(bookmarked_fandoms, result_dict_fandoms):
    # Filter common fandoms
    common_fandoms = [fandom for fandom in result_dict_fandoms if fandom in bookmarked_fandoms]
    return common_fandoms

def add_common_fandoms_to_result_dict(result_dict, bookmarked_fandoms):
    for author_info in result_dict.values():
        result_dict_fandoms = author_info['fandoms']
        common_fandoms = find_common_fandoms(bookmarked_fandoms, result_dict_fandoms)
        author_info['fandoms_of_interest'] = common_fandoms

def get_most_recent_publish_date(works):
    # Find the most recent publish date among the works
    most_recent_publish_date = None
    for work in works:
        if most_recent_publish_date is None or work.date_updated > most_recent_publish_date:
            most_recent_publish_date = work.date_updated
    return most_recent_publish_date

def export(fandoms):
    json_object = json.dumps(fandoms, indent = 4)
    return json_object
    #json useful for prettyprinting
    #but a csv or google sheet is probably ultimately the most useful
    #if csv:
        #create a new one every time? with timestamped filenames & including the session/bookmark data username?
        #or new rows and include the time stamp & session date as columns?
    #if google sheet:
        #already feels weird enough working on this on work time
        #but it is indeed good practice
        #still, wouldn't want that saved in my work account and I feel weird about using my personal instead
        #hm


#run in order
def run():
    session = start()

    subscribed_authors = get_subscribed_authors(session)

    bookmarks = get_bookmarks(session)

    bookmarked_authors = get_bookmarked_authors(bookmarks)

    authors_messy = merge_authors(subscribed_authors, bookmarked_authors)

    authors = filter_unique(authors_messy)

    from pickling import pickle_path
    #the pickling will need to be rewritten, that's the older version
    fandoms = get_author_fandoms(authors, pickle_path)
    
    export(fandoms)